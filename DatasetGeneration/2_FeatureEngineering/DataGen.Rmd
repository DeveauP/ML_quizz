---
title: '"Feature engineering"'
author: "Paul Deveau"
date: "`r Sys.Date()`"
output: 
html_document:
  highlight: tango
  theme: cosmo
self_contained: no
fig_width: 7
fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
rows = 5*10**4 
library(ggplot2)

library(cowplot)
```

This time, we'll assess the capability of dealing with feature engineering.

For the first dataset, we will create a very simple relation between x,y and z:
$z = x + y^2 + e$
Where e is random noise.

```{r dataset1}
x<-rnorm(n = rows,mean = 25, sd = 9)
y<-rnorm(n = rows,mean = 5, sd = 3)
z<-x+y**2 + runif(n = rows)

```

We save the dataset

```{r shuffle1}

df<-data.frame(x= x,y= y,z =z)

write.table(x = df, file = "dataset2.csv",col.names = TRUE, row.names = FALSE,sep=",")

```

To check our data, we plot it:
```{r plots1}
df$zp<-df$x+df$y**2
xp<-ggplot(df,aes(x = x,y= z))+geom_point(alpha = I(0.01))
yp<-ggplot(df,aes(x = y,y= z))+geom_point(alpha = I(0.01))
zp<-ggplot(df,aes(x = zp,y= z))+geom_point(alpha = I(0.01))
plot_grid(xp,yp,zp,align = "v",ncol = 3)

```

We can clearly see the noise with $zp =f(z)$ , but plotting $z = f(x)$ or $z = f(y)$ is not very useful, except to shown that z is function of x and y.

When solving this kind of problem, a first sanity check is to make sure that x and y are not correlated! (I won't check it here, maybe in the correction if I make one...)

So let's start with correlation:
```{r correlation}
cor(x = df$x, y = df$z, method = "spearman") # test rank correlation
cor(x = df$y, y = df$z, method = "spearman") # test rank correlation

reg1<-lm(z ~ x+y,data = df)
reg1
```
So we can see that x and z are positively correlated, as well as x and z. 

Is this model any good?
```{r cortest1}
cor.test(x = reg1$coefficients["x"] * df$x+ reg1$coefficients["y"]* df$y, y = df$z, method = "pearson") # test linear correlation

```

With an $r^2$ of 0.9258083 , we can make better than that!

A good thing to estimate the coefficient of a polynom is the log/log graph (the slope when x is large gives you the maximal coefficient). Remember that semi log scales are used to find exponential laws. Here, x values can be negative, so we ofset by min(x) +1 .
```{r loglog}
logdf<-df
logdf$x<-logdf$x - min(logdf$x)+1
logdf$y<-logdf$y - min(logdf$y)+1
logdf$z<-logdf$z - min(logdf$z)+1

logdf<-log(logdf)
xp<-ggplot(logdf,aes(x = x,y= z))+geom_point(alpha = I(0.1))
yp<-ggplot(logdf,aes(x = y,y= z))+geom_point(alpha = I(0.1))

plot_grid(xp,yp,ncol = 2)
```

Now we can see that log(z) seems to be linear with log(y) for large log(y), so we try the simplest polynom in y:

```{r pol2}
df$y2<-df$y**2

reg2<-lm(z ~ x + y +y2,data = df)
cor.test(x = reg2$coefficients["x"] * df$x+ reg2$coefficients["y"]* df$y + reg2$coefficients["y2"]* df$y2, y = df$z, method = "pearson") # test linear correlation


```

Almost 1 correlation, so far so good, but we test on the train sample! Not good!

```{r pol3}

split<-sample(1:nrow(df),size = round(nrow(df)/2))
train<-df[split,]
test<-df[-split,]
reg1<-lm(z ~ x + y +y2,data = train)
cor.test(x = reg1$coefficients["x"] * test$x+ reg1$coefficients["y"]* test$y, y = test$z, method = "pearson")
reg2<-lm(z ~ x + y +y2,data = train)
cor.test(x = reg2$coefficients["x"] * test$x+ reg2$coefficients["y"]* test$y + reg2$coefficients["y2"]*test$y2, y = test$z, method = "pearson")


```
See the difference for the case 1: the correlation is now 0.257, while our second model still is doing good!

We conclude by printing the parameters of our final model:

```{r modelparams}
reg2
```

Notice that the intercept is almost 0.5 (that is the mean of the uniform distribution across $[0;1]$ !)
